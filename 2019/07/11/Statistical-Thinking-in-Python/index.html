<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Python," />










<meta name="description" content="Learn how to think and analyze statistically with Python (Datacamp).">
<meta name="keywords" content="Python">
<meta property="og:type" content="article">
<meta property="og:title" content="Statistical Thinking in Python">
<meta property="og:url" content="https://joannaoyzl.github.io/2019/07/11/Statistical-Thinking-in-Python/index.html">
<meta property="og:site_name" content="Joanna">
<meta property="og:description" content="Learn how to think and analyze statistically with Python (Datacamp).">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-07-17T01:17:52.324Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Statistical Thinking in Python">
<meta name="twitter:description" content="Learn how to think and analyze statistically with Python (Datacamp).">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://joannaoyzl.github.io/2019/07/11/Statistical-Thinking-in-Python/"/>





  <title>Statistical Thinking in Python | Joanna</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-134453862-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Joanna</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://joannaoyzl.github.io/2019/07/11/Statistical-Thinking-in-Python/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joanna">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joanna">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Statistical Thinking in Python</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-11T13:58:45-07:00">
                2019-07-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Course-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Course Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2019/07/11/Statistical-Thinking-in-Python/" class="leancloud_visitors" data-flag-title="Statistical Thinking in Python">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Learn how to think and analyze statistically with Python (Datacamp).<br><a id="more"></a></p>
<h1 id="Exploratory-Data-Analysis-EDA"><a href="#Exploratory-Data-Analysis-EDA" class="headerlink" title="Exploratory Data Analysis (EDA)"></a>Exploratory Data Analysis (EDA)</h1><h2 id="Graphical-EDA"><a href="#Graphical-EDA" class="headerlink" title="Graphical EDA"></a>Graphical EDA</h2><p>We usually use <code>matplotlib.pyplot</code> and <code>seaborn</code> to draw easy graphs to help our understanding of the dataset.</p>
<p>Key Takeaways:</p>
<ul>
<li>Use <code>Seaborn</code> style to make the graph look prettier.</li>
<li>Always attach labels to the axes.</li>
</ul>
<h3 id="Histogram"><a href="#Histogram" class="headerlink" title="Histogram"></a>Histogram</h3><p>Key takeaways:</p>
<ul>
<li>Number of bins is the <strong>square root</strong> of number of data points.</li>
<li>2 drawbacks:<ul>
<li><strong>Bining bias</strong>: the same dat amay be interpreted differently depending on choice of bins</li>
<li>We are not plotting all of the data, since they are swept into bins and we lost their actual values. </li>
<li>A <strong>Bee swarm plot</strong> can help with the above drawbacks.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using seaborn to set default style</span></span><br><span class="line">sns.set() </span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute number of data points: n_data</span></span><br><span class="line">n_data = len(versicolor_petal_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of bins is the square root of number of data points: n_bins</span></span><br><span class="line">n_bins = int(np.sqrt(n_data))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">_ = plt.hist(versicolor_petal_length, bins = n_bins) <span class="comment"># a numpy array</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Label axes</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'petal length (cm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'count'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show histogram</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Bee-Swarm-Plot"><a href="#Bee-Swarm-Plot" class="headerlink" title="Bee Swarm Plot"></a>Bee Swarm Plot</h3><p>Key Takeaways:</p>
<ul>
<li>Organize your data into <strong>pandas dataframe</strong> before drawing the plot.</li>
<li>You need to specify x and y, and also the dataframe you are using when drawing the plot.</li>
<li>A good way to distingush the distribution of data from different categories.</li>
<li><strong>Limitation:</strong> When there are way too many data points, it tend to obsculate the data<ul>
<li>We can solve this by <strong>Empirical Cumulative Distribution Function (ECDF)</strong></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create bee swarm plot with Seaborn's default settings</span></span><br><span class="line">_ = sns.swarmplot(x = <span class="string">'species'</span>, y = <span class="string">'petal length (cm)'</span>, data = df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label the axes</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'species'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'petal length (cm)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Empirical-Cumulative-Distribution-Function-ECDF"><a href="#Empirical-Cumulative-Distribution-Function-ECDF" class="headerlink" title="Empirical Cumulative Distribution Function (ECDF)"></a>Empirical Cumulative Distribution Function (ECDF)</h3><p>Basics：</p>
<ul>
<li><strong>X-value</strong> is the quantity you are measuring.<ul>
<li>The x-values are the sorted data. Use the <code>np.sort()</code> function to perform the sorting.</li>
</ul>
</li>
<li><strong>Y-value</strong> is the fraction of data points that have a value <strong>smaller</strong> than the corresponding x-value.<ul>
<li>The y data of the ECDF go from <code>1/n</code> to <code>1</code> in equally spaced increments. You can construct this using <code>np.arange()</code>. Remember, however, that the end value in <code>np.arange()</code> is not inclusive. Therefore, <code>np.arange()</code> will need to go from <code>1</code> to <code>n+1</code>. Be sure to divide this by <code>n</code>.</li>
</ul>
</li>
<li>It can not only show all the data, but also give you a compleyte picture of how data are distributed.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the ECDF function to be reused in the future</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ecdf</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">"""Compute ECDF for a one-dimensional array of measurements."""</span></span><br><span class="line">    <span class="comment"># Number of data points: n</span></span><br><span class="line">    n = len(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x-data for the ECDF: x</span></span><br><span class="line">    x = np.sort(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># y-data for the ECDF: y</span></span><br><span class="line">    y = np.arange(<span class="number">1</span>, n + <span class="number">1</span>) / n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute ECDFs</span></span><br><span class="line">x_set, y_set = ecdf(setosa_petal_length)</span><br><span class="line">x_vers, y_vers = ecdf(versicolor_petal_length)</span><br><span class="line">x_virg, y_virg = ecdf(virginica_petal_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot all ECDFs on the same plot, only the dots in line shape but without the line</span></span><br><span class="line">_ = plt.plot(x_set, y_set, marker = <span class="string">'.'</span>, linestyle = <span class="string">'none'</span>)</span><br><span class="line">_ = plt.plot(x_vers, y_vers, marker = <span class="string">'.'</span>, linestyle = <span class="string">'none'</span>)</span><br><span class="line">_ = plt.plot(x_virg, y_virg, marker = <span class="string">'.'</span>, linestyle = <span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Annotate the plot</span></span><br><span class="line">plt.legend((<span class="string">'setosa'</span>, <span class="string">'versicolor'</span>, <span class="string">'virginica'</span>), loc=<span class="string">'lower right'</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">'petal length (cm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'ECDF'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Scatter-Plot"><a href="#Scatter-Plot" class="headerlink" title="Scatter Plot"></a>Scatter Plot</h3><p>When you want to compare two variables, we can use scatter plot.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make a scatter plot</span></span><br><span class="line">_ = plt.plot(versicolor_petal_length, versicolor_petal_width, marker = <span class="string">'.'</span>, linestyle = <span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label the axes</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'petal length (cm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'petal width (cm)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the result</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h2 id="Quantitative-EDA"><a href="#Quantitative-EDA" class="headerlink" title="Quantitative EDA"></a>Quantitative EDA</h2><h3 id="Mean-amp-Median"><a href="#Mean-amp-Median" class="headerlink" title="Mean &amp; Median"></a>Mean &amp; Median</h3><ul>
<li>Mean:<ul>
<li>Easy to compute</li>
<li>Heavily influenced by outliers</li>
<li><code>np.mean()</code></li>
</ul>
</li>
<li>Median:<ul>
<li>The middle value of a data set</li>
<li>Immune to outliers</li>
<li><code>np.median()</code></li>
</ul>
</li>
</ul>
<h3 id="Percentiles"><a href="#Percentiles" class="headerlink" title="Percentiles"></a>Percentiles</h3><ul>
<li>Percentiles<ul>
<li><code>np.percentile(, [25, 50. 75])</code> we need to enter the percentiles that we want into this function</li>
<li><strong>Boxplot</strong> is helpful in visualizing it.<ul>
<li>Middle line is the median (50th percentile), and the edge of the box is the 25th and 75th percentile. The length of the box is <strong>IQR (Inter Quartile Range).</strong></li>
<li>The streched out line is either located at <strong>1.5 IQR</strong> or the extent of the data.</li>
<li>The data points that fall outside the two lines are outliers, since it is common practice to <strong>regard all the data points that fall 2 IQR away from median as outliers</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create box plot with Seaborn's default settings</span></span><br><span class="line">_ = sns.boxplot(x = <span class="string">'species'</span>, y = <span class="string">'petal length (cm)'</span>, data = df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label the axes</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'species'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'petal length (cm)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>You can also plot the percentiles directly onto ECDF with red markers.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify array of percentiles: percentiles</span></span><br><span class="line">percentiles = np.array([<span class="number">2.5</span>, <span class="number">25</span>, <span class="number">50</span>, <span class="number">75</span>, <span class="number">97.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute percentiles: ptiles_vers</span></span><br><span class="line">ptiles_vers = np.percentile(versicolor_petal_length, percentiles)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the ECDF</span></span><br><span class="line">_ = plt.plot(x_vers, y_vers, <span class="string">'.'</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">'petal length (cm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'ECDF'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Overlay percentiles as red diamonds.</span></span><br><span class="line">_ = plt.plot(ptiles_vers, percentiles/<span class="number">100</span>, marker=<span class="string">'D'</span>, color=<span class="string">'red'</span>,</span><br><span class="line">         linestyle=<span class="string">'none'</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="Variance-and-Standard-Deviation"><a href="#Variance-and-Standard-Deviation" class="headerlink" title="Variance and Standard Deviation"></a>Variance and Standard Deviation</h3><ul>
<li><strong>Variance</strong><ul>
<li>The mean squared distance of the data from their mean, or informally, a measure of the spread of data.</li>
</ul>
</li>
</ul>
<p>Calculation of variance:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Array of differences to mean: differences</span></span><br><span class="line">differences = versicolor_petal_length - np.mean(versicolor_petal_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Square the differences: diff_sq</span></span><br><span class="line">diff_sq = differences**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the mean square difference: variance_explicit</span></span><br><span class="line">variance_explicit = np.mean(diff_sq)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the variance using NumPy: variance_np</span></span><br><span class="line">variance_np = np.var(versicolor_petal_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the results (they are the same)</span></span><br><span class="line">print(variance_explicit, variance_np)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>Standard Deviation</strong><ul>
<li>The square root of variance</li>
</ul>
</li>
</ul>
<p>Calculation of Standard Deviation:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute the variance: variance</span></span><br><span class="line">variance = np.var(versicolor_petal_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the square root of the variance</span></span><br><span class="line">print(np.sqrt(variance))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the standard deviation</span></span><br><span class="line">print(np.std(versicolor_petal_length))</span><br></pre></td></tr></table></figure></p>
<h3 id="Covariance-and-Pearson-Correlation-Coefficient"><a href="#Covariance-and-Pearson-Correlation-Coefficient" class="headerlink" title="Covariance and Pearson Correlation Coefficient"></a>Covariance and Pearson Correlation Coefficient</h3><ul>
<li><p><strong>Covariance</strong></p>
<ul>
<li>A measure of how two quantities vary together</li>
<li>$covariance = \frac{1}{n}\sum\limits_{i=1}^n(x_i - \overline{x})(y_i - \overline{y})$</li>
<li>For most of the data points, if their differences to the two quantities’ means are all in the same direction (either all greater than the mean or all smaller than the mean) , then <strong>the covariance is postive</strong>, and thus the two quantities are positively or negatively correlated.</li>
<li>To avoid the measure having any units, we use the <strong>Person Correlation Coefficient</strong></li>
<li><p>Covariance Matrix:</p>
<ul>
<li><p>The covariance may be computed using the Numpy function <code>np.cov()</code>. For example, we have two sets of data x and y, <code>np.cov(x, y)</code> returns a 2D array where entries <code>[0,1]</code> and <code>[1,0]</code> are the covariances. Entry <code>[0,0]</code> is the variance of the data in x, and entry <code>[1,1]</code> is the variance of the data in y. This 2D output array is called the covariance matrix, since it organizes the self- and covariance.</p>
<p>|  | 0  |  1 |<br>|—  |—|—|<br>|0  |variance of x |<strong>covariance</strong>   |<br>|1  |<strong>covariance</strong>    |variance of y|</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute the covariance matrix: covariance_matrix</span></span><br><span class="line">covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print covariance matrix</span></span><br><span class="line">print(covariance_matrix)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Extract covariance of length and width of petals: petal_cov</span></span><br><span class="line">petal_cov = covariance_matrix[<span class="number">0</span>,<span class="number">1</span>] <span class="comment">#[1,0] is the same</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the length/width covariance</span></span><br><span class="line">print(petal_cov)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>Person Correlation Coefficient</strong></p>
<ul>
<li>$\rho = \text{Person Correlation Coefficient} = \frac{\text{covariance}}{(\text{std of x})(\text{std of y})} = \frac{\text{variability due to codependence}}{\text{independent variability}}$</li>
<li>Range from -1 <em>(complete negative correlation)</em> to 1 <em>(complete positive correlation)</em>. 0 means there is no correlation at all between these two variables.</li>
<li>It is computed using the <code>np.corrcoef()</code> function. Like <code>np.cov()</code>, it takes two arrays as arguments and returns a 2D array. Entries <code>[0,0]</code> and <code>[1,1]</code> are necessarily equal to 1, and the value we are after is entry <code>[0,1]</code>.</li>
<li><code>array([[1.        , 0.78666809], [0.78666809, 1.        ]])</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearson_r</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">"""Compute Pearson correlation coefficient between two arrays."""</span></span><br><span class="line">    <span class="comment"># Compute correlation matrix: corr_mat</span></span><br><span class="line">    corr_mat = np.corrcoef(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return entry [0,1]</span></span><br><span class="line">    <span class="keyword">return</span> corr_mat[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute Pearson correlation coefficient for I. versicolor: r</span></span><br><span class="line">r = pearson_r(versicolor_petal_length, versicolor_petal_width)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the result</span></span><br><span class="line">print(r)</span><br></pre></td></tr></table></figure>
<h1 id="Probablistic-Logic"><a href="#Probablistic-Logic" class="headerlink" title="Probablistic Logic"></a>Probablistic Logic</h1><p>Probability provides a measure of uncertainty. Data are almost never exactly the same when acquired again, and probability allows us to say how much we expect them to vary.</p>
<p>Statistical inference involves taking your data to probabilistic conclusions about what you would expect if you took even more data, and you can make decisions based on these conclusions.</p>
<h2 id="Discrete-Variables"><a href="#Discrete-Variables" class="headerlink" title="Discrete Variables"></a>Discrete Variables</h2><p><strong>Hacker Statistics</strong><br>Hacker statistics uses simulated repeated measurements to compute probabilities. The logic is as simple as follows:</p>
<ol>
<li>Determine how to simulate data</li>
<li>Simulate many many times</li>
<li>Probability is approximately fraction of trails with the outcome of interest.</li>
</ol>
<h3 id="Bernoulli-Trial"><a href="#Bernoulli-Trial" class="headerlink" title="Bernoulli Trial"></a>Bernoulli Trial</h3><p>Bernoulli trial is an experiment that has two options, “success” (True) and “failure” (False).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a Bernoulli Trial function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perform_bernoulli_trials</span><span class="params">(n, p)</span>:</span></span><br><span class="line">    <span class="string">"""Perform n Bernoulli trials with success probability p</span></span><br><span class="line"><span class="string">    and return number of successes."""</span></span><br><span class="line">    <span class="comment"># Initialize number of successes: n_success</span></span><br><span class="line">    n_success = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Perform trials</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># Choose random number between zero and one: random_number</span></span><br><span class="line">        random_number = np.random.random()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If less than p, it's a success so add one to n_success</span></span><br><span class="line">        <span class="keyword">if</span> random_number &lt; p:</span><br><span class="line">            n_success += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> n_success</span><br></pre></td></tr></table></figure>
<p><strong>EXAMPLE</strong><br>Follwing is an example using bernoulli trials:<br><em>Let’s say a bank made 100 mortgage loans. It is possible that anywhere between 0 and 100 of the loans will be defaulted upon. You would like to know the probability of getting a given number of defaults, given that the probability of a default is p = 0.05. To investigate this, you will do a simulation. You will perform 100 Bernoulli trials using the <code>perform_bernoulli_trials()</code> function you wrote and record how many defaults we get. Here, a success is a default. (Remember that the word “success” just means that the Bernoulli trial evaluates to True, i.e., did the loan recipient default?) You will do this for another 100 Bernoulli trials. And again and again until we have tried it 1000 times. Then, you will plot a histogram describing the probability of the number of defaults.</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Seed random number generator</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the number of defaults: n_defaults</span></span><br><span class="line">n_defaults = np.empty(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the number of defaults</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    n_defaults[i] = perform_bernoulli_trials(<span class="number">100</span>, <span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the histogram with default number of bins; label your axes</span></span><br><span class="line">_ = plt.hist(n_defaults, normed = <span class="keyword">True</span>) </span><br><span class="line"><span class="comment"># Include the normed = True keyword argument </span></span><br><span class="line"><span class="comment"># so that the height of the bars of the histogram indicate the probability.</span></span><br><span class="line"></span><br><span class="line">_ = plt.xlabel(<span class="string">'number of defaults out of 100 loans'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'probability'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>The above code performs the same way as <code>np.random.binomial(n, p, size = x)</code>, just broke it down into more detailde steps. x is the number of for loops.</p>
<p><em>If interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability that the bank will lose money?</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute ECDF: x, y</span></span><br><span class="line">x, y = ecdf(n_defaults)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the ECDF with labeled axes</span></span><br><span class="line">_ = plt.plot(x, y, marker = <span class="string">'.'</span>, linestyle = <span class="string">'none'</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">'number of defaults out of 100'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'CDF'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money</span></span><br><span class="line">n_lose_money = np.sum(n_defaults &gt;= <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and print probability of losing money</span></span><br><span class="line">print(<span class="string">'Probability of losing money ='</span>, n_lose_money / len(n_defaults))</span><br></pre></td></tr></table></figure>
<h3 id="Binomial-Distribution"><a href="#Binomial-Distribution" class="headerlink" title="Binomial Distribution"></a>Binomial Distribution</h3><p><strong>Probability Mass Function (PMF)</strong> is the set of probabilities of <em>discrete</em> outcomes.</p>
<p>When you are rolling a dice, the outcome is discrete, with the same probability, and therefore it has a discrete uniform PMF.</p>
<p><strong>Probability Distribution</strong> is a mathematical description of outcomes.</p>
<p><strong>Binomial Distribution</strong>: the number $r$ of sucesses in $n$ Bernoulli trials with probability $p$ of success, is Binomially distributed. <code>np.random.binomial(n, p, size = x)</code></p>
<h3 id="Poisson-Distribution"><a href="#Poisson-Distribution" class="headerlink" title="Poisson Distribution"></a>Poisson Distribution</h3><p><strong>Poisson process</strong>: the timing of the next event is completely independent of when the previous event happened.</p>
<ul>
<li>Examples:<ul>
<li>Natural births in a given hospital</li>
<li>Hit on a website during a given hour</li>
<li>Metror strikes</li>
<li>Molecular collisions in a gas</li>
<li>Aviation incidents</li>
<li>Buses in Poissonville</li>
</ul>
</li>
</ul>
<p>The number of arrivals of a Poisson process in a given amount of time is <strong>Poisson distributed</strong>.</p>
<p><strong>Poisson distribution</strong>: </p>
<ul>
<li>The number $r$ of arrivals of a Poisson process <em>in a given time interval</em> with average rate of $\lambda$ arrivals per interval is Poisson distributed. <ul>
<li>One parameter: $\lambda$, the average times of arrival in a given time.</li>
<li>Example:<ul>
<li>The number $r$ of hits on a website in one hour with an average hit rate of 6 hits per hour is Poisson distributed. <code>np.random.poisson(6, size = 100000)</code></li>
</ul>
</li>
</ul>
</li>
<li>It is the <strong>limit</strong> of the Binomial distribution for <strong>low probability of success</strong> and <strong>large number of trials</strong>. (rare events)<ul>
<li><strong>The logic behind:</strong> This makes sense if you think about the stories. Say we do a Bernoulli trial every minute for an hour, each with a success probability of 0.1. We would do 60 trials, and the number of successes is Binomially distributed, and we would expect to get about 6 successes. This is just like the Poisson story we discussed in the video, where we get on average 6 hits on a website per hour. So, <em>the Poisson distribution with arrival rate equal to $np$ approximates a Binomial distribution for $n$ Bernoulli trials with probability $p$ of success (with $n$ large and $p$ small).</em> Importantly, <strong>the Poisson distribution is often simpler to work with because it has only one parameter instead of two for the Binomial distribution.</strong></li>
</ul>
</li>
</ul>
<p>The code below emprically proved that Poisson distribution is the limit of Binomial distribution for <strong>low probability of success</strong> and <strong>large number of trials</strong>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Draw 10,000 samples out of Poisson distribution: samples_poisson</span></span><br><span class="line">samples_poisson = np.random.poisson(<span class="number">10</span>, size = <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the mean and standard deviation</span></span><br><span class="line">print(<span class="string">'Poisson:     '</span>, np.mean(samples_poisson),</span><br><span class="line">                       np.std(samples_poisson))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify values of n and p to consider for Binomial: n, p</span></span><br><span class="line">n = [<span class="number">20</span>, <span class="number">100</span>, <span class="number">1000</span>]</span><br><span class="line">p = [<span class="number">0.5</span>, <span class="number">0.1</span>, <span class="number">0.01</span>] <span class="comment"># so np is always 10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Draw 10,000 samples for each n,p pair: samples_binomial</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    samples_binomial = np.random.binomial(n[i], p[i], size = <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print results</span></span><br><span class="line">    print(<span class="string">'n ='</span>, n[i], <span class="string">'Binom:'</span>, np.mean(samples_binomial),</span><br><span class="line">                                 np.std(samples_binomial))</span><br></pre></td></tr></table></figure></p>
<h2 id="Continuous-Variables"><a href="#Continuous-Variables" class="headerlink" title="Continuous Variables"></a>Continuous Variables</h2><h3 id="Probability-Density-Function-PDF"><a href="#Probability-Density-Function-PDF" class="headerlink" title="Probability Density Function (PDF)"></a>Probability Density Function (PDF)</h3><ul>
<li>Continuous analog to the Probability Mass Function (PMF).</li>
<li>Mathematical description of the relative likelihood of observing a value of continuous variable.</li>
<li>Can be visualized by setting <code>normed = True</code> when ploting histogram, since the total area under the curve should be 1.</li>
<li>The probability of observing a single value dos not make sense, since there is an infinity of numbers. Therefore, the area under the PDF give probabilities. For example, the probability of observing a value greater than x value is $p$, which is $p*100$% of the total area under the PDF.</li>
<li>Therefore, we are actually calculating the cumulative distribution function, or CDF, of the distribution curve.</li>
</ul>
<h3 id="Normal-Gaussian-Distribution"><a href="#Normal-Gaussian-Distribution" class="headerlink" title="Normal (Gaussian) Distribution"></a>Normal (Gaussian) Distribution</h3><p><strong>Normal Distribution</strong> describes a continuous variable whose PDF has a single symmetric peak.</p>
<ul>
<li>Two true parameters of the distribution: (Make sure not to confuse with the mean and std calculated from the data)<ul>
<li><strong>Mean</strong>, determines where the center of the peak is.</li>
<li><strong>Standard deviation</strong>, a measure of how wide the oeaj is, or how spread out the data are.</li>
</ul>
</li>
</ul>
<p>We can check whether a dataset is similar to a normal distribution by comparing its empirical CDF with the theoretical normal CDF (generated by <code>np.random.normal()</code> using directly the data’s mean and std).<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute mean and standard deviation: mu, sigma</span></span><br><span class="line">mu = np.mean(belmont_no_outliers)</span><br><span class="line">sigma = np.std(belmont_no_outliers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample out of a normal distribution with this mu and sigma: samples</span></span><br><span class="line">samples = np.random.normal(mu, sigma, size = <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the CDF of the samples and of the data</span></span><br><span class="line">x, y = ecdf(belmont_no_outliers)</span><br><span class="line">x_theor, y_theor = ecdf(samples)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the CDFs and show the plot</span></span><br><span class="line">_ = plt.plot(x_theor, y_theor)</span><br><span class="line">_ = plt.plot(x, y, marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">'Belmont winning time (sec.)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'CDF'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>The code below shows the normal curves of distributions with the same mean but different std. The higher the std, the flatter and shorter of the curve.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10</span></span><br><span class="line">samples_std1 = np.random.normal(<span class="number">20</span>, <span class="number">1</span>, size = <span class="number">100000</span>)</span><br><span class="line">samples_std3 = np.random.normal(<span class="number">20</span>, <span class="number">3</span>, size = <span class="number">100000</span>)</span><br><span class="line">samples_std10 = np.random.normal(<span class="number">20</span>, <span class="number">10</span>, size = <span class="number">100000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make histograms</span></span><br><span class="line">_ = plt.hist(samples_std1, bins = <span class="number">100</span>, normed = <span class="keyword">True</span>, histtype = <span class="string">'step'</span>)</span><br><span class="line">_ = plt.hist(samples_std3, bins = <span class="number">100</span>, normed = <span class="keyword">True</span>, histtype = <span class="string">'step'</span>)</span><br><span class="line">_ = plt.hist(samples_std10, bins = <span class="number">100</span>, normed = <span class="keyword">True</span>, histtype = <span class="string">'step'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a legend, set limits and show plot</span></span><br><span class="line">_ = plt.legend((<span class="string">'std = 1'</span>, <span class="string">'std = 3'</span>, <span class="string">'std = 10'</span>))</span><br><span class="line">plt.ylim(<span class="number">-0.01</span>, <span class="number">0.42</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate CDFs</span></span><br><span class="line">x_std1, y_std1 = ecdf(samples_std1)</span><br><span class="line">x_std3, y_std3 = ecdf(samples_std3)</span><br><span class="line">x_std10, y_std10 = ecdf(samples_std10)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot CDFs</span></span><br><span class="line">_ = plt.plot(x_std1, y_std1, marker = <span class="string">'.'</span>, linestyle = <span class="string">'none'</span>)</span><br><span class="line">_ = plt.plot(x_std3, y_std3, marker = <span class="string">'.'</span>, linestyle = <span class="string">'none'</span>)</span><br><span class="line">_ = plt.plot(x_std10, y_std10, marker = <span class="string">'.'</span>, linestyle = <span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a legend and show the plot</span></span><br><span class="line">_ = plt.margins(<span class="number">0.02</span>) <span class="comment"># so that there can be a margin near the plot frame</span></span><br><span class="line">_ = plt.legend((<span class="string">'std = 1'</span>, <span class="string">'std = 3'</span>, <span class="string">'std = 10'</span>), loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>Key Things to Keep in Mind</strong><ul>
<li><strong>Do not easily assume normality!</strong> Use histogram and CDF to help you make a better judgement.</li>
<li>Normal distribution has very light tail, which means the probability of heing &gt;4std away from the mean is very small. Therefore, when you are modeling data as normally distributed, <strong>outliers are extremely unlikely</strong>. But real life data often have extreme values.</li>
</ul>
</li>
</ul>
<h3 id="Exponential-Distribution"><a href="#Exponential-Distribution" class="headerlink" title="Exponential Distribution"></a>Exponential Distribution</h3><p><strong>The Exponential Distribution</strong>: the waiting time between arrivals of a Poisson process is exponentially distributed. In other words, it describes the wating time between rare events.</p>
<ul>
<li>One parameter: the average waiting time.</li>
</ul>
<p><strong>EXAMPLE</strong></p>
<p><em>In earlier exercises, we looked at the rare event of no-hitters in Major League Baseball. Hitting the cycle is another rare baseball event. When a batter hits the cycle, he gets all four kinds of hits, a single, double, triple, and home run, in a single game. Like no-hitters, this can be modeled as a Poisson process, so the time between hits of the cycle are also Exponentially distributed.</em></p>
<p><em>How long must we wait to see both a no-hitter and then a batter hit the cycle? The idea is that we have to wait some time for the no-hitter, and then after the no-hitter, we have to wait for hitting the cycle. Stated another way, what is the total waiting time for the arrival of two different Poisson processes? The total waiting time is the time waited for the no-hitter, plus the time waited for the hitting the cycle.</em></p>
<p><em>Now, you will write a function to sample out of the distribution described by this story.</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">successive_poisson</span><span class="params">(tau1, tau2, size=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Compute time for arrival of 2 successive Poisson processes."""</span></span><br><span class="line">    <span class="comment"># Draw samples out of first exponential distribution: t1</span></span><br><span class="line">    t1 = np.random.exponential(tau1, size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Draw samples out of second exponential distribution: t2</span></span><br><span class="line">    t2 = np.random.exponential(tau2, size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> t1 + t2</span><br></pre></td></tr></table></figure>
<p><em>Now, you’ll use your sampling function to compute the waiting time to observe a no-hitter and hitting of the cycle. The mean waiting time for a no-hitter is 764 games, and the mean waiting time for hitting the cycle is 715 games.</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Draw samples of waiting times: waiting_times</span></span><br><span class="line">waiting_times = successive_poisson(<span class="number">764</span>, <span class="number">715</span>, size = <span class="number">100000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make the histogram</span></span><br><span class="line">_ = plt.hist(waiting_times, bins = <span class="number">100</span>, normed = <span class="keyword">True</span>, histtype = <span class="string">'step'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label axes</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'total waiting time (games)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'PDF'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h1><p><strong>Optimal parameters</strong> are values that bring the model in closest agreement with the data.<br>They are only optimal for the model you chose for your data. And thus if your model is wrong, the optimal parameters are not really meaningful.</p>
<p>Generate theoretical data follows exponential distribution<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Seed random number generator</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute mean no-hitter time: tau</span></span><br><span class="line">tau = np.mean(nohitter_times)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Draw out of an exponential distribution with parameter tau: inter_nohitter_time</span></span><br><span class="line">inter_nohitter_time = np.random.exponential(tau, <span class="number">100000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the PDF and label axes</span></span><br><span class="line">_ = plt.hist(inter_nohitter_time,</span><br><span class="line">             bins = <span class="number">50</span>, histtype = <span class="string">'step'</span>, normed = <span class="keyword">True</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">'Games between no-hitters'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'PDF'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>Overlay theoretical CDF with empirical CDF<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create an ECDF from real data: x, y</span></span><br><span class="line">x, y = ecdf(nohitter_times)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a CDF from theoretical samples: x_theor, y_theor</span></span><br><span class="line">x_theor, y_theor = ecdf(inter_nohitter_time)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Overlay the plots</span></span><br><span class="line">plt.plot(x_theor, y_theor)</span><br><span class="line">plt.plot(x, y, marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Margins and axis labels</span></span><br><span class="line">plt.margins(<span class="number">0.02</span>) <span class="comment"># so that there can be a margin near the plot frame</span></span><br><span class="line">plt.xlabel(<span class="string">'Games between no-hitters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'CDF'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h2 id="Linear-Regression-by-Least-Sqaures"><a href="#Linear-Regression-by-Least-Sqaures" class="headerlink" title="Linear Regression by Least Sqaures"></a>Linear Regression by Least Sqaures</h2><p>Often, we can use a linear function to describe the pattern of the data.</p>
<ul>
<li>Two parameters:<ul>
<li><strong>Slope</strong>: how steep the line is </li>
<li><strong>Intercept</strong>: where the line crosses the y-axis<br>We want to choose the slope and intercept that data points collectively lie as close as possible to the line</li>
</ul>
</li>
</ul>
<p><strong>Residual</strong> is the vertical distance (not the perpendicular) distance between the data point and the line. It is negative when the point lies below the line. Each data point has a residual associated with it.</p>
<p><strong>Least squares</strong> is the process of finding the parameters for which the <strong>sum of the squares of the residuals</strong> is minimal. There are many algorithms that can be used to perform this process. Here, we will use the Numpy function <code>np.polyfit(x, y, polynomial_degree)</code>, which performs least squares analysis with polynomial functions. Simply set the polynomial degree to 1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the illiteracy rate versus fertility</span></span><br><span class="line">_ = plt.plot(illiteracy, fertility, marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>)</span><br><span class="line">plt.margins(<span class="number">0.02</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">'percent illiterate'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'fertility'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform a linear regression using np.polyfit(): a, b</span></span><br><span class="line">a, b = np.polyfit(illiteracy, fertility, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the results to the screen</span></span><br><span class="line">print(<span class="string">'slope ='</span>, a, <span class="string">'children per woman / percent illiterate'</span>)</span><br><span class="line">print(<span class="string">'intercept ='</span>, b, <span class="string">'children per woman'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make theoretical line to plot</span></span><br><span class="line">x = np.array([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line">y = a * x + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add regression line to your plot</span></span><br><span class="line">_ = plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Draw the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>The details behind <code>np.polyfit()</code> function is as follows: it tries to find the minimal residual by testing various combinations.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify slopes to consider: a_vals</span></span><br><span class="line">a_vals = np.linspace(<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize sum of square of residuals: rss</span></span><br><span class="line">rss = np.empty_like() <span class="comment">#returns a new array with the same shape and type as a given array (in this case, a_vals).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute sum of square of residuals for each value of a_vals</span></span><br><span class="line"><span class="keyword">for</span> i, a <span class="keyword">in</span> enumerate(a_vals): <span class="comment"># enumerate will return (0, a), (1, b), which means add index to value, and wrap them in tuple</span></span><br><span class="line">    rss[i] = np.sum((fertility - a * illiteracy - b)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the RSS</span></span><br><span class="line">plt.plot(a_val, rss, <span class="string">'-'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'slope (children per woman / percent illiterate)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'sum of square of residuals'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="Bootstrap"><a href="#Bootstrap" class="headerlink" title="Bootstrap"></a>Bootstrap</h1><p><strong>Bootstrapping</strong> is the use of resampled data to perform statistical inference. It resamples data with replacement to the same size as the original data.</p>
<ul>
<li><strong>Bootstrap Sample</strong>: A resampled array pf data</li>
<li><strong>Bootstrap Replicate</strong>: A statistic computed from a <em>bootstrap sample</em>.</li>
<li>We can use <code>np.random.choice([list], size = len([list]))</code> to get a bootstrap sample, and then compute a bootstrap replicate.</li>
</ul>
<h2 id="Bootstrap-Confidence-Interval"><a href="#Bootstrap-Confidence-Interval" class="headerlink" title="Bootstrap Confidence Interval"></a>Bootstrap Confidence Interval</h2><p><strong>Confidence Interval</strong>: If we repeat the measurement over and over again, $p$% of the observed values should lie within the $p$% confidence interval. </p>
<ul>
<li>95% CI: <code>np.percentiles(, [2.5, 97.5])</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bootstrap_replicate_1d</span><span class="params">(data, func)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> func(np.random.choice(data, size=len(data)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_bs_reps</span><span class="params">(data, func, size=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Draw bootstrap replicates."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize array of replicates: bs_replicates</span></span><br><span class="line">    bs_replicates = np.empty(size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Generate replicates</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">        bs_replicates[i] = bootstrap_replicate_1d(data, func)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bs_replicates</span><br><span class="line"></span><br><span class="line"><span class="comment"># Take 10,000 bootstrap replicates of the mean: bs_replicates</span></span><br><span class="line">bs_replicates = draw_bs_reps(rainfall, np.mean, size = <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and print SEM</span></span><br><span class="line">sem = np.std(rainfall) / np.sqrt(len(rainfall))</span><br><span class="line">print(sem)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and print standard deviation of bootstrap replicates</span></span><br><span class="line">bs_std = np.std(bs_replicates)</span><br><span class="line">print(bs_std) <span class="comment"># proved to be the same as the original std</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a histogram of the results</span></span><br><span class="line">_ = plt.hist(bs_replicates, bins=<span class="number">50</span>, normed=<span class="keyword">True</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">'mean annual rainfall (mm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'PDF'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show() <span class="comment"># showed the distribution of the bootstrap replicates of the mean is Normal</span></span><br></pre></td></tr></table></figure>
<h2 id="Pairs-Bootstrap"><a href="#Pairs-Bootstrap" class="headerlink" title="Pairs Bootstrap"></a>Pairs Bootstrap</h2><p><strong>Non-parametric Inference</strong>: Make no assumptions about the model or probability distribution underlying the data.</p>
<p>But for fitting a linear regression model to existing data, we already have two parameters. Thus, here we want to perform a <strong>parametric estimate</strong> by using <strong>pairs bootstrap</strong>.</p>
<ul>
<li>Resample data <em>in pairs</em>. (Since there are two variables).</li>
<li>Compute slope and interceptfrom resampled data.</li>
<li>Each slope and intercept is a bootstrap replicate.</li>
<li>Compute confidence intervals from percentiles of bootstrap replicates.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_bs_pairs_linreg</span><span class="params">(x, y, size=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Perform pairs bootstrap for linear regression."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set up array of indices to sample from: inds</span></span><br><span class="line">    inds = np.arange(len(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize replicates: bs_slope_reps, bs_intercept_reps</span></span><br><span class="line">    bs_slope_reps = np.empty(size)</span><br><span class="line">    bs_intercept_reps = np.empty(size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Generate replicates</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">        bs_inds = np.random.choice(inds, size = len(x))</span><br><span class="line">        bs_x, bs_y = x[bs_inds], y[bs_inds]</span><br><span class="line">        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bs_slope_reps, bs_intercept_reps</span><br></pre></td></tr></table></figure>
<p>Using the function you just wrote, perform pairs bootstrap to plot a histogram describing the estimate of the slope from the illiteracy/fertility data. Also report the 95% confidence interval of the slope. The data is available to you in the NumPy arrays illiteracy and fertility.</p>
<p>As a reminder, draw_bs_pairs_linreg() has a function signature of draw_bs_pairs_linreg(x, y, size=1), and it returns two values: bs_slope_reps and bs_intercept_reps.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate replicates of slope and intercept using pairs bootstrap</span></span><br><span class="line">bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size = <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and print 95% CI for slope</span></span><br><span class="line">print(np.percentile(bs_slope_reps, [<span class="number">2.5</span>, <span class="number">97.5</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the histogram</span></span><br><span class="line">_ = plt.hist(bs_slope_reps, bins=<span class="number">50</span>, normed=<span class="keyword">True</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">'slope'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'PDF'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>A nice way to visualize the variability we might expect in a linear regression is to plot the line you would get from each bootstrap replicate of the slope and intercept.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate array of x-values for bootstrap lines: x</span></span><br><span class="line">x = np.array([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the bootstrap lines</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    _ = plt.plot(x, </span><br><span class="line">                 bs_slope_reps[i] * x + bs_intercept_reps[i],</span><br><span class="line">                 linewidth=<span class="number">0.5</span>, alpha=<span class="number">0.2</span>, color=<span class="string">'red'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the data</span></span><br><span class="line">_ = plt.plot(illiteracy, fertility, marker = <span class="string">'.'</span>, linestyle = <span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label axes, set the margins, and show the plot</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'illiteracy'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'fertility'</span>)</span><br><span class="line">plt.margins(<span class="number">0.02</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h1 id="Hypothesis-Testing"><a href="#Hypothesis-Testing" class="headerlink" title="Hypothesis Testing"></a>Hypothesis Testing</h1><p>How to assess how reasonable it is that our observed data are actually described by the model? This is when <strong>hypothesis testing</strong> come into play.</p>
<p><strong>Hypothesis Testing</strong>: Assessment of how reasonable the observed data are assuming the hypothesis is true. The hypothesis we are testing are called <strong>Null Hypothesis</strong>.</p>
<p>To simulate a null hypothesis, where we assume two quantities are identically distributed, we use <strong>Permutation</strong> to randomly reorder entries in an array.</p>
<ul>
<li>Put two sets of data, A (a number of data points)and B (b number of data points), together, and randomly shuffle them, ignoring their origins. </li>
<li>Then label the first a shuffled data points as A, the rest as B.</li>
<li>Then we draw the two ecdf, and repeat the above process again and again to generate multiple permutation samples.</li>
<li>We can compare the new ecdf with the original one to see whether the two overlaps. If not, then the two distributions are not identical.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Write a function to resample the data with permutation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">permutation_sample</span><span class="params">(data1, data2)</span>:</span></span><br><span class="line">    <span class="string">"""Generate a permutation sample from two data sets."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Concatenate the data sets: data</span></span><br><span class="line">    data = np.concatenate((data1, data2)) <span class="comment">#needs to be put in a tuple</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Permute the concatenated array: permuted_data</span></span><br><span class="line">    permuted_data = np.random.permutation(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Split the permuted array into two: perm_sample_1, perm_sample_2</span></span><br><span class="line">    perm_sample_1 = permuted_data[:len(data1)]</span><br><span class="line">    perm_sample_2 = permuted_data[len(data1):]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> perm_sample_1, perm_sample_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize Permutated sample distribution</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">    <span class="comment"># Generate permutation samples</span></span><br><span class="line">    perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute ECDFs</span></span><br><span class="line">    x_1, y_1 = ecdf(perm_sample_1)</span><br><span class="line">    x_2, y_2 = ecdf(perm_sample_2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot ECDFs of permutation sample</span></span><br><span class="line">    _ = plt.plot(x_1, y_1, marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>,</span><br><span class="line">                 color=<span class="string">'red'</span>, alpha=<span class="number">0.02</span>)</span><br><span class="line">    _ = plt.plot(x_2, y_2, marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>,</span><br><span class="line">                 color=<span class="string">'blue'</span>, alpha=<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and plot ECDFs from original data</span></span><br><span class="line">x_1, y_1 = ecdf(rain_june)</span><br><span class="line">x_2, y_2 = ecdf(rain_november)</span><br><span class="line">_ = plt.plot(x_1, y_1, marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">_ = plt.plot(x_2, y_2, marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>, color=<span class="string">'blue'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label axes, set margin, and show plot</span></span><br><span class="line">plt.margins(<span class="number">0.02</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">'monthly rainfall (mm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'ECDF'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><em>Notice that the permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that <strong>the hypothesis is not commensurate with the data</strong>. July and November rainfall are not identically distributed.</em></p>
<h2 id="Test-Statistic"><a href="#Test-Statistic" class="headerlink" title="Test Statistic"></a>Test Statistic</h2><p><strong>Test Statistic</strong>:</p>
<ul>
<li>is a single number that can be computed from observed data and from data you simulate under the null hypothesis. </li>
<li>It serves as a basis of comparison between what the hypotheiss predicts and what we actually observed.</li>
<li>You should choose a test statistic that is <strong>pertinent to the question you are trying to answer</strong>. e.g. Whether the two states’ vote distribution are identical? If the are identical, their mean should be the same. Therefore, we can choose the difference in means as our test statsitic.</li>
<li>If combined with permutation techniques, the calculated statistics can be referred to as <strong>permutation replicates</strong>.</li>
</ul>
<p>If we generated enough permutation sample, we can draw a histogram of the permutaion replicates. Here we need to introduce the idea of p-value.</p>
<p><strong>p-value</strong>: </p>
<ul>
<li>is the probability of observing a test statistic <strong>equally or more extreme</strong> than the one you observed, <em>given that the null hypothesis is true.</em></li>
<li><strong>It is NOT the probability that the null hypothesis is true.</strong></li>
<li>If it is small, then we say that the data are statistically significant to prove that the null hypothesis is not true.</li>
<li>Also, <strong>statistical significance $\neq$ practical significance</strong>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_perm_reps</span><span class="params">(data_1, data_2, func, size=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Generate multiple permutation replicates."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize array of replicates: perm_replicates</span></span><br><span class="line">    perm_replicates = np.empty(size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">        <span class="comment"># Generate permutation sample</span></span><br><span class="line">        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the test statistic</span></span><br><span class="line">        perm_replicates[i] = func(perm_sample_1, perm_sample_2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> perm_replicates</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">diff_of_means</span><span class="params">(data_1, data_2)</span>:</span></span><br><span class="line">    <span class="string">"""Difference in means of two arrays."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># The difference of means of data_1, data_2: diff</span></span><br><span class="line">    diff = np.mean(data_1) - np.mean(data_2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute difference of mean impact force from experiment: empirical_diff_means</span></span><br><span class="line">empirical_diff_means = diff_of_means(force_a, force_b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Draw 10,000 permutation replicates: perm_replicates</span></span><br><span class="line">perm_replicates = draw_perm_reps(force_a, force_b,</span><br><span class="line">                                 diff_of_means, size=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute p-value: p</span></span><br><span class="line">p = np.sum(perm_replicates &gt;= empirical_diff_means) / len(perm_replicates)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the result</span></span><br><span class="line">print(<span class="string">'p-value ='</span>, p)</span><br></pre></td></tr></table></figure>
<p><em>The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. A p-value below 0.01 is typically said to be “statistically significant,” but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be “statistically significant,” but they are definitely not the same!</em></p>
<p><strong>Summary of hypothesis testing pipeline</strong></p>
<ul>
<li>Clearly state the null hypothesis <strong>(do EDA BEFORE forming Null hypothesis!)</strong></li>
<li>Define your test statistic</li>
<li>Generate many sets of simulated data assuming the null hypothesis is true</li>
<li>Compute the test statistic for each simulated data set</li>
<li>The p-value is the fraction of your simulated data sets for which the test statistic is at leas as extreme as for the real data</li>
</ul>
<p><strong><em>EXAMPLE: A one-sample bootstrap hypothesis test</em></strong><br><em>Another juvenile frog was studied, Frog C, and you want to see if Frog B and Frog C have similar impact forces. Unfortunately, you do not have Frog C’s impact forces available, but you know they have a mean of 0.55 N. Because you don’t have the original data, you cannot do a permutation test, and you cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. You will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C.</em></p>
<p><em>To set up the bootstrap hypothesis test, you will take the mean as our test statistic. Remember, your goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B’s impact forces is equal to that of Frog C is true. You first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B’s distribution, such as the variance, unchanged.</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make an array of translated impact forces: translated_force_b</span></span><br><span class="line">force_c_mean = <span class="number">0.55</span></span><br><span class="line">translated_force_b = force_b - np.mean(force_b) + force_c_mean</span><br><span class="line"></span><br><span class="line"><span class="comment"># Take bootstrap replicates of Frog B's translated impact forces: bs_replicates</span></span><br><span class="line">bs_replicates = draw_bs_reps(translated_force_b, np.mean, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute fraction of replicates that are less than the observed Frog B force: p</span></span><br><span class="line">p = np.sum(bs_replicates &lt;= np.mean(force_b)) / <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the p-value</span></span><br><span class="line">print(<span class="string">'p = '</span>, p)</span><br></pre></td></tr></table></figure></p>
<p><em>The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false.</em></p>
<p><strong><em>EXAMPLE: A two-sample bootstrap hypothesis test for difference of means</em></strong><br><em>We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test.</em></p>
<p><em>To do the two-sample bootstrap test, <strong>we shift both arrays to have the same mean</strong>, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed.</em></p>
<p><em>The objects <code>forces_concat</code> and <code>empirical_diff_means</code> are already in your namespace.</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute mean of all forces: mean_force</span></span><br><span class="line">mean_force = np.mean(forces_concat)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate shifted arrays</span></span><br><span class="line">force_a_shifted = force_a - np.mean(force_a) + mean_force</span><br><span class="line">force_b_shifted = force_b - np.mean(force_b) + mean_force</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute 10,000 bootstrap replicates from shifted arrays</span></span><br><span class="line">bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, <span class="number">10000</span>)</span><br><span class="line">bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get replicates of difference of means: bs_replicates</span></span><br><span class="line">bs_replicates = bs_replicates_a - bs_replicates_b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and print p-value: p</span></span><br><span class="line">p = np.sum(bs_replicates &gt;= (np.mean(force_a) - np.mean(force_b))) / len(bs_replicates)</span><br><span class="line">print(<span class="string">'p-value ='</span>, p) <span class="comment">#0.0043</span></span><br></pre></td></tr></table></figure></p>
<p><em>You got a similar result as when you did the permutation test. Nonetheless, remember that it is important to carefully think about what question you want to ask. Are you only interested in the mean impact force, or in the distribution of impact forces?</em></p>
<h2 id="A-B-Testing"><a href="#A-B-Testing" class="headerlink" title="A/B Testing"></a>A/B Testing</h2><p>Often, we want to know whether the difference of the CTR of the two web designs are entirely due to random chance, or statistically significant. Therefore, we want to check what is the probability of observing a difference <strong>equally or more extreme</strong> than the one you observed, given that we assume the difference is entirely due to chance, i.e, the p-value.</p>
<p>We can use permutation here to simulate as if the redesign had no effect on the CTR.</p>
<p><strong>A/B Testing</strong> </p>
<ul>
<li>is often used by organizations to see if a change in strategy gives difference, hopefully better, results.</li>
<li>The null hypothesis of an A/B test is often: the test statistic is impervious to the change. A low p-value implies that the change in strategy lead to a change in performance.</li>
</ul>
<p><strong><em>EXAMPLE: The vote for the Civil Rights Act in 1964</em></strong></p>
<p><em>The Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding “present” and “abstain” votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote?</em></p>
<p><em>To answer this question, you will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. You will use the fraction of Democrats voting in favor as your test statistic and evaluate the probability of observing a fraction of Democrats voting in favor <strong>at least as small as</strong> the observed fraction of 153/244. (That’s right, at least as small as. In 1964, <strong>it was the Democrats who were less progressive on civil rights issues.</strong>) To do this, permute the party labels of the House voters and then arbitrarily divide them into “Democrats” and “Republicans” and compute the fraction of Democrats voting yea.</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Construct arrays of data: dems, reps</span></span><br><span class="line">dems = np.array([<span class="keyword">True</span>] * <span class="number">153</span> + [<span class="keyword">False</span>] * <span class="number">91</span>)</span><br><span class="line">reps = np.array([<span class="keyword">True</span>] * <span class="number">136</span> + [<span class="keyword">False</span>] * <span class="number">35</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">frac_yea_dems</span><span class="params">(dems, reps)</span>:</span> <span class="comment">#Two inputs are required to use the below draw_perm_reps() function, but the second is not used.</span></span><br><span class="line">    <span class="string">"""Compute fraction of Democrat yea votes."""</span></span><br><span class="line">    frac = np.sum(dems) / len(dems)</span><br><span class="line">    <span class="keyword">return</span> frac</span><br><span class="line"></span><br><span class="line"><span class="comment"># Acquire permutation samples: perm_replicates</span></span><br><span class="line">perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and print p-value: p</span></span><br><span class="line">p = np.sum(perm_replicates &lt;= <span class="number">153</span>/<span class="number">244</span>) / len(perm_replicates)</span><br><span class="line">print(<span class="string">'p-value ='</span>, p) <span class="comment">#p-value = 0.0002</span></span><br></pre></td></tr></table></figure>
<p><em>This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias.</em></p>
<h2 id="Test-of-Correlation"><a href="#Test-of-Correlation" class="headerlink" title="Test of Correlation"></a>Test of Correlation</h2><p>How can we know whether the correlation is true, or only happens by chance? We can do a hypothesis test.</p>
<ul>
<li>Null hypothesis: the two variables are completely uncorrelated.</li>
<li>Simulate data assuming null hypothesis is true. (shuffle the x and y pairs entirely to cut any possible inferred correlation)</li>
<li>Use Pearson correlation, $\rho$, as test statsitic.</li>
<li>Compute p-value as fraction of replicates that have $\rho$ at least as large as observed.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute observed correlation: r_obs (using pearson_r() that was wrote before)</span></span><br><span class="line">r_obs = pearson_r(illiteracy, fertility)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize permutation replicates: perm_replicates</span></span><br><span class="line">perm_replicates = np.empty(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Draw replicates</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    <span class="comment"># Permute illiteracy measurments: illiteracy_permuted</span></span><br><span class="line">    illiteracy_permuted = np.random.permutation(illiteracy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Pearson correlation</span></span><br><span class="line">    perm_replicates[i] = pearson_r(illiteracy_permuted, fertility)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute p-value: p</span></span><br><span class="line">p = np.sum(perm_replicates &gt;= r_obs) / len(perm_replicates)</span><br><span class="line">print(<span class="string">'p-val ='</span>, p) <span class="comment">#p-val = 0.0</span></span><br></pre></td></tr></table></figure>
<p>If there are no replicates that is at least as large as observed, it does not mean that the p-value is zero, but means that it needs an enormous amount of replicates to get just one that is extreme enough as specified. Therefore, the p-value is extremely small.</p>
<h1 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h1><h2 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h2><p><strong>EDA of beak depth</strong><br>For your first foray into the Darwin finch data, you will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species Geospiza scandens has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, Geospiza fortis. These effects can lead to changes in the species over time.</p>
<p>In the next few problems, you will look at the beak depth of G. scandens on Daphne Major in 1975 and in 2012. To start with, let’s plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot.</p>
<p>The data are stored in a pandas DataFrame called df with columns <code>&#39;year&#39;</code> and <code>&#39;beak_depth&#39;</code>. The units of beak depth are millimeters (mm).<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create bee swarm plot</span></span><br><span class="line">_ = sns.swarmplot(x = <span class="string">'year'</span>, y = <span class="string">'beak_depth'</span>, data = df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label the axes</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'year'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'beak depth (mm)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><strong>ECDFs of beak depths</strong><br>While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA. Plot the ECDFs for the 1975 and 2012 beak depth measurements on the same plot.</p>
<p>For your convenience, the beak depths for the respective years has been stored in the NumPy arrays <code>bd_1975</code> and <code>bd_2012</code>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute ECDFs</span></span><br><span class="line">x_1975, y_1975 = ecdf(bd_1975)</span><br><span class="line">x_2012, y_2012 = ecdf(bd_2012)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the ECDFs</span></span><br><span class="line">_ = plt.plot(x_1975, y_1975, marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>)</span><br><span class="line">_ = plt.plot(x_2012, y_2012, marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set margins</span></span><br><span class="line">plt.margins(<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add axis labels and legend</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'beak depth (mm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'ECDF'</span>)</span><br><span class="line">_ = plt.legend((<span class="string">'1975'</span>, <span class="string">'2012'</span>), loc=<span class="string">'lower right'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><strong>Parameter estimates of beak depths</strong><br>Estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval.</p>
<p>Since in this exercise you will use the draw_bs_reps() function you wrote in chapter 2, it may be helpful to refer back to it.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute the difference of the sample means: mean_diff</span></span><br><span class="line">mean_diff = np.mean(bd_2012) - np.mean(bd_1975)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get bootstrap replicates of means</span></span><br><span class="line">bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, <span class="number">10000</span>)</span><br><span class="line">bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute samples of difference of means: bs_diff_replicates</span></span><br><span class="line">bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute 95% confidence interval: conf_int</span></span><br><span class="line">conf_int = np.percentile(bs_diff_replicates, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the results</span></span><br><span class="line">print(<span class="string">'difference of means ='</span>, mean_diff, <span class="string">'mm'</span>)</span><br><span class="line">print(<span class="string">'95% confidence interval ='</span>, conf_int, <span class="string">'mm'</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>Hypothesis test: Are beaks deeper in 2012?</strong><br>Your plot of the ECDF and determination of the confidence interval make it pretty clear that the beaks of G. scandens on Daphne Major have gotten deeper. But is it possible that this effect is just due to random chance? In other words, what is the probability that we would get the observed difference in mean beak depth if the means were the same?</p>
<p>Be careful! The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets so that they have the same mean and then use bootstrap sampling to compute the difference of means.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute mean of combined data set: combined_mean</span></span><br><span class="line">combined_mean = np.mean(np.concatenate((bd_1975, bd_2012)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Shift the samples</span></span><br><span class="line">bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean</span><br><span class="line">bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get bootstrap replicates of shifted data sets</span></span><br><span class="line">bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, <span class="number">10000</span>)</span><br><span class="line">bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute replicates of difference of means: bs_diff_replicates</span></span><br><span class="line">bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the p-value</span></span><br><span class="line">p = np.sum(bs_diff_replicates &gt;= np.mean(bd_2012) - np.mean(bd_1975)) / len(bs_diff_replicates)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print p-value</span></span><br><span class="line">print(<span class="string">'p ='</span>, p)</span><br></pre></td></tr></table></figure>
<p><strong>EDA of beak length and depth</strong><br>The beak length data are stored as <code>bl_1975</code> and <code>bl_2012</code>, again with units of millimeters (mm). You still have the beak depth data stored in bd_1975 and bd_2012. Make scatter plots of beak depth (y-axis) versus beak length (x-axis) for the 1975 and 2012 specimens.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make scatter plot of 1975 data</span></span><br><span class="line">_ = plt.plot(bl_1975, bd_1975, marker=<span class="string">'.'</span>,</span><br><span class="line">             linestyle=<span class="string">'None'</span>, color = <span class="string">'blue'</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make scatter plot of 2012 data</span></span><br><span class="line">_ = plt.plot(bl_2012, bd_2012, marker=<span class="string">'.'</span>,</span><br><span class="line">            linestyle=<span class="string">'None'</span>, color = <span class="string">'red'</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label axes and make legend</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'beak length (mm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'beak depth (mm)'</span>)</span><br><span class="line">_ = plt.legend((<span class="string">'1975'</span>, <span class="string">'2012'</span>), loc=<span class="string">'upper left'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Linear-regressions"><a href="#Linear-regressions" class="headerlink" title="Linear regressions"></a>Linear regressions</h2><p>Perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line.</p>
<p>You will use the <code>draw_bs_pairs_linreg()</code> function you wrote back in chapter 2.</p>
<p>As a reminder, its call signature is <code>draw_bs_pairs_linreg(x, y, size=1)</code>, and it returns <code>bs_slope_reps</code> and <code>bs_intercept_reps</code>. The beak length data are stored as <code>bl_1975</code> and <code>bl_2012</code>, and the beak depth data is stored in <code>bd_1975</code> and <code>bd_2012</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute the linear regressions</span></span><br><span class="line">slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, <span class="number">1</span>)</span><br><span class="line">slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform pairs bootstrap for the linear regressions</span></span><br><span class="line">bs_slope_reps_1975, bs_intercept_reps_1975 = \</span><br><span class="line">        draw_bs_pairs_linreg(bl_1975, bd_1975, <span class="number">1000</span>)</span><br><span class="line">bs_slope_reps_2012, bs_intercept_reps_2012 = \</span><br><span class="line">        draw_bs_pairs_linreg(bl_2012, bd_2012, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute confidence intervals of slopes</span></span><br><span class="line">slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line">slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line">intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line">intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the results</span></span><br><span class="line">print(<span class="string">'1975: slope ='</span>, slope_1975,</span><br><span class="line">      <span class="string">'conf int ='</span>, slope_conf_int_1975)</span><br><span class="line">print(<span class="string">'1975: intercept ='</span>, intercept_1975,</span><br><span class="line">      <span class="string">'conf int ='</span>, intercept_conf_int_1975)</span><br><span class="line">print(<span class="string">'2012: slope ='</span>, slope_2012,</span><br><span class="line">      <span class="string">'conf int ='</span>, slope_conf_int_2012)</span><br><span class="line">print(<span class="string">'2012: intercept ='</span>, intercept_2012,</span><br><span class="line">      <span class="string">'conf int ='</span>, intercept_conf_int_2012)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make scatter plot of 1975 data</span></span><br><span class="line">_ = plt.plot(bl_1975, bd_1975, marker=<span class="string">'.'</span>,</span><br><span class="line">             linestyle=<span class="string">'none'</span>, color=<span class="string">'blue'</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make scatter plot of 2012 data</span></span><br><span class="line">_ = plt.plot(bl_2012, bd_2012, marker=<span class="string">'.'</span>,</span><br><span class="line">             linestyle=<span class="string">'none'</span>, color=<span class="string">'red'</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label axes and make legend</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'beak length (mm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'beak depth (mm)'</span>)</span><br><span class="line">_ = plt.legend((<span class="string">'1975'</span>, <span class="string">'2012'</span>), loc=<span class="string">'upper left'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate x-values for bootstrap lines: x</span></span><br><span class="line">x = np.array([<span class="number">10</span>, <span class="number">17</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the bootstrap lines</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    plt.plot(x, bs_slope_reps_1975[i] * x + bs_intercept_reps_1975[i],</span><br><span class="line">             linewidth=<span class="number">0.5</span>, alpha=<span class="number">0.2</span>, color=<span class="string">'blue'</span>)</span><br><span class="line">    plt.plot(x, bs_slope_reps_2012[i] * x + bs_intercept_reps_2012[i],</span><br><span class="line">             linewidth=<span class="number">0.5</span>, alpha=<span class="number">0.2</span>, color=<span class="string">'red'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Draw the plot again</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>The linear regressions showed interesting information about the beak geometry. The slope was the same in 1975 and 2012, suggesting that for every millimeter gained in beak length, the birds gained about half a millimeter in depth in both years. However, if we are interested in the shape of the beak, we want to compare the ratio of beak length to beak depth. Let’s make that comparison.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute length-to-depth ratios</span></span><br><span class="line">ratio_1975 = bl_1975 / bd_1975</span><br><span class="line">ratio_2012 = bl_2012 / bd_2012</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute means</span></span><br><span class="line">mean_ratio_1975 = np.mean(ratio_1975)</span><br><span class="line">mean_ratio_2012 = np.mean(ratio_2012)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate bootstrap replicates of the means</span></span><br><span class="line">bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, <span class="number">10000</span>)</span><br><span class="line">bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the 99% confidence intervals</span></span><br><span class="line">conf_int_1975 = np.percentile(bs_replicates_1975, [<span class="number">0.5</span>, <span class="number">99.5</span>])</span><br><span class="line">conf_int_2012 = np.percentile(bs_replicates_2012, [<span class="number">0.5</span>, <span class="number">99.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the results</span></span><br><span class="line">print(<span class="string">'1975: mean ratio ='</span>, mean_ratio_1975,</span><br><span class="line">      <span class="string">'conf int ='</span>, conf_int_1975)</span><br><span class="line">print(<span class="string">'2012: mean ratio ='</span>, mean_ratio_2012,</span><br><span class="line">      <span class="string">'conf int ='</span>, conf_int_2012)</span><br></pre></td></tr></table></figure>
<p>To determine whether this is a real effect or just due to noise, we need to further compute the p-value.</p>
<h2 id="EDA-of-heritability"><a href="#EDA-of-heritability" class="headerlink" title="EDA of heritability"></a>EDA of heritability</h2><p>The array <code>bd_parent_scandens</code> contains the average beak depth (in mm) of two parents of the species G. scandens. The array <code>bd_offspring_scandens</code> contains the average beak depth of the offspring of the respective parents. The arrays <code>bd_parent_fortis</code> and <code>bd_offspring_fortis</code> contain the same information about measurements from G. fortis birds.</p>
<p>Make a scatter plot of the average offspring beak depth (y-axis) versus average parental beak depth (x-axis) for both species. Use the <code>alpha=0.5</code> keyword argument to help you see overlapping points.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make scatter plots</span></span><br><span class="line">_ = plt.plot(bd_parent_fortis, bd_offspring_fortis,</span><br><span class="line">             marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>, color=<span class="string">'blue'</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">_ = plt.plot(bd_parent_scandens, bd_offspring_scandens,</span><br><span class="line">             marker=<span class="string">'.'</span>, linestyle=<span class="string">'none'</span>, color=<span class="string">'red'</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Label axes</span></span><br><span class="line">_ = plt.xlabel(<span class="string">'parental beak depth (mm)'</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">'offspring beak depth (mm)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add legend</span></span><br><span class="line">_ = plt.legend((<span class="string">'G. fortis'</span>, <span class="string">'G. scandens'</span>), loc=<span class="string">'lower right'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plot</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>It appears as though there is a stronger correlation in G. fortis than in G. scandens. This suggests that beak depth is more strongly inherited in G. fortis. We’ll quantify this correlation next.</p>
<p>In an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap.</p>
<p>You have already written a function to do pairs bootstrap to get estimates for parameters derived from linear regression. Your task in this exercise is to make a new function with call signature <code>draw_bs_pairs(x, y, func, size=1)</code> that performs pairs bootstrap and computes a single statistic, Pearson correlation on pairs samples defined. </p>
<p>Compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens. Do the same for G. fortis. Then, use the function you wrote in the last exercise to compute a 95% confidence interval using pairs bootstrap.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_bs_pairs</span><span class="params">(x, y, func, size=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Perform pairs bootstrap for a single statistic."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set up array of indices to sample from: inds</span></span><br><span class="line">    inds = np.arange(len(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize replicates: bs_replicates</span></span><br><span class="line">    bs_replicates = np.empty(size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Generate replicates</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">        bs_inds = np.random.choice(inds, size = len(inds))</span><br><span class="line">        bs_x, bs_y = x[bs_inds], y[bs_inds]</span><br><span class="line">        bs_replicates[i] = func(bs_x, bs_y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bs_replicates</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the Pearson correlation coefficients</span></span><br><span class="line">r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens)</span><br><span class="line">r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Acquire 1000 bootstrap replicates of Pearson r</span></span><br><span class="line">bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute 95% confidence intervals</span></span><br><span class="line">conf_int_scandens = np.percentile(bs_replicates_scandens, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line">conf_int_fortis = np.percentile(bs_replicates_fortis, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print results</span></span><br><span class="line">print(<span class="string">'G. scandens:'</span>, r_scandens, conf_int_scandens)</span><br><span class="line">print(<span class="string">'G. fortis:'</span>, r_fortis, conf_int_fortis)</span><br><span class="line"></span><br><span class="line"><span class="comment">#    G. scandens: 0.4117063629401258 [0.26564228 0.54388972]</span></span><br><span class="line"><span class="comment">#    G. fortis: 0.7283412395518487 [0.6694112  0.77840616]</span></span><br></pre></td></tr></table></figure>
<p>It is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts.</p>
<p>Remember that the Pearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets. This is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, <strong>it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone</strong>. In this exercise, you will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval.</p>
<p>This exercise highlights a very important point. Statistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heritability</span><span class="params">(parents, offspring)</span>:</span></span><br><span class="line">    <span class="string">"""Compute the heritability from parent and offspring samples."""</span></span><br><span class="line">    covariance_matrix = np.cov(parents, offspring)</span><br><span class="line">    <span class="keyword">return</span> covariance_matrix[<span class="number">0</span>, <span class="number">1</span>] / covariance_matrix[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the heritability</span></span><br><span class="line">heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens)</span><br><span class="line">heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Acquire 1000 bootstrap replicates of heritability</span></span><br><span class="line">replicates_scandens = draw_bs_pairs(</span><br><span class="line">        bd_parent_scandens, bd_offspring_scandens, heritability, size=<span class="number">1000</span>)</span><br><span class="line">        </span><br><span class="line">replicates_fortis = draw_bs_pairs(</span><br><span class="line">        bd_parent_fortis, bd_offspring_fortis, heritability, size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute 95% confidence intervals</span></span><br><span class="line">conf_int_scandens = np.percentile(replicates_scandens, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line">conf_int_fortis = np.percentile(replicates_fortis, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print results</span></span><br><span class="line">print(<span class="string">'G. scandens:'</span>, heritability_scandens, conf_int_scandens)</span><br><span class="line">print(<span class="string">'G. fortis:'</span>, heritability_fortis, conf_int_fortis)</span><br></pre></td></tr></table></figure>
<p>Here again, we see that G. fortis has stronger heritability than G. scandens. This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization.</p>
<p><strong>Is beak depth heritable at all in G. scandens?</strong><br>The heritability of beak depth in G. scandens seems low. It could be that this observed heritability was just achieved by chance and beak depth is actually not really heritable in the species. You will test that hypothesis here. To do this, you will do a pairs permutation test.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize array of replicates: perm_replicates</span></span><br><span class="line">perm_replicates = np.empty(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Draw replicates</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    <span class="comment"># Permute parent beak depths</span></span><br><span class="line">    bd_parent_permuted = np.random.permutation(bd_parent_scandens)</span><br><span class="line">    perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute p-value: p</span></span><br><span class="line">p = np.sum(perm_replicates &gt;= heritability_scandens) / len(perm_replicates)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the p-value</span></span><br><span class="line">print(<span class="string">'p-val ='</span>, p) <span class="comment">#p-val = 0.0</span></span><br></pre></td></tr></table></figure>
<p>You get a p-value of zero, which means that none of the 10,000 permutation pairs replicates you drew had a heritability high enough to match that which was observed. This strongly suggests that <strong>beak depth is heritable in G. scandens, just not as much as in G. fortis.</strong> If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/10/Relational-DB-in-SQL/" rel="next" title="Introduction to Relational Databases in SQL">
                <i class="fa fa-chevron-left"></i> Introduction to Relational Databases in SQL
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/16/Intro-to-Shell-for-DS/" rel="prev" title="Introduction to Shell for Data Science">
                Introduction to Shell for Data Science <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview sidebar-nav-active" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Joanna" />
            
              <p class="site-author-name" itemprop="name">Joanna</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Exploratory-Data-Analysis-EDA"><span class="nav-text">Exploratory Data Analysis (EDA)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Graphical-EDA"><span class="nav-text">Graphical EDA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Histogram"><span class="nav-text">Histogram</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bee-Swarm-Plot"><span class="nav-text">Bee Swarm Plot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Empirical-Cumulative-Distribution-Function-ECDF"><span class="nav-text">Empirical Cumulative Distribution Function (ECDF)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scatter-Plot"><span class="nav-text">Scatter Plot</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Quantitative-EDA"><span class="nav-text">Quantitative EDA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mean-amp-Median"><span class="nav-text">Mean &amp; Median</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Percentiles"><span class="nav-text">Percentiles</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Variance-and-Standard-Deviation"><span class="nav-text">Variance and Standard Deviation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Covariance-and-Pearson-Correlation-Coefficient"><span class="nav-text">Covariance and Pearson Correlation Coefficient</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Probablistic-Logic"><span class="nav-text">Probablistic Logic</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Discrete-Variables"><span class="nav-text">Discrete Variables</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Bernoulli-Trial"><span class="nav-text">Bernoulli Trial</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Binomial-Distribution"><span class="nav-text">Binomial Distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Poisson-Distribution"><span class="nav-text">Poisson Distribution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Continuous-Variables"><span class="nav-text">Continuous Variables</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Probability-Density-Function-PDF"><span class="nav-text">Probability Density Function (PDF)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal-Gaussian-Distribution"><span class="nav-text">Normal (Gaussian) Distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exponential-Distribution"><span class="nav-text">Exponential Distribution</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Parameters"><span class="nav-text">Parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Regression-by-Least-Sqaures"><span class="nav-text">Linear Regression by Least Sqaures</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bootstrap"><span class="nav-text">Bootstrap</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Bootstrap-Confidence-Interval"><span class="nav-text">Bootstrap Confidence Interval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pairs-Bootstrap"><span class="nav-text">Pairs Bootstrap</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hypothesis-Testing"><span class="nav-text">Hypothesis Testing</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Test-Statistic"><span class="nav-text">Test Statistic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-B-Testing"><span class="nav-text">A/B Testing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Test-of-Correlation"><span class="nav-text">Test of Correlation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Case-Study"><span class="nav-text">Case Study</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#EDA"><span class="nav-text">EDA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-regressions"><span class="nav-text">Linear regressions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EDA-of-heritability"><span class="nav-text">EDA of heritability</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joanna</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a></div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("sEEYEudBHdtybAJf5AQkiOl2-gzGzoHsz", "cDbeqqCJt4dq4hu7C9GaCrHB");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
